{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Newton's Method Optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary packages\n",
    "import torch\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Step 1 - `newton()` function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following `newton()` function demonstrates Newtonâ€™s method for minimization of functions using PyTorch and autograd functionality.  I mainly utilized two iterations to accomplish the task: one outer iteration which minimizes the objective function value with parameters upgrade, and one inner iteration which halves `Newton step` to ensure reduction in the objective function value. Before two iterations, the initial parameters are transformed into `torch` data types and whether there are finite objective or derivatives is checked. First, in the outer iteration, `Jacobian` and `Hessian` values are computed to derive `Newton step`. `Hessian` values are checked to be positive definite and if not, they are perturbed by adding multiples of the identity matrix. Second, in the inner iteration, parameters and objective function value are upgraded with `Newton step`. Whether `Newton step` reduces the objective function value is checked and if not, `Newton step` is halved. An error is raised if `Newton step` fails to reduce the objective function value with maximum step halving number. Third, back to the outer iteration, parameters, objective function value, iterations number and Jacobian values are upgraded. A specific condition for convergence is checked and an error is raised if failing to converge within maximum iterations number. In addition, whether `Hessian` value is positive definite at convergence is checked. Eventually, the `newton()` function returns a dictionary containing: the minimum objective function value, the parameters values, the iterations number and the gradient vector at the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton(theta, f, tol=1e-8, fscale=1.0, maxit=100, max_half=20):\n",
    "    '''\n",
    "    The function minimizes objective function value with Newton's method.\n",
    "    Input arguments:\n",
    "    theta: vector of initial values for optimization parameters\n",
    "    f: objective function to minimize\n",
    "    tol: convergence tolerance\n",
    "    fscale: rough estimate of the magnitude of f at the optimum\n",
    "    maxit: maximum number of Newton iterations to try before giving up\n",
    "    max_half: maximum number of times a step should be halved before concluding failure\n",
    "    Output:\n",
    "    f: value of the objective function at the minimum\n",
    "    theta: value of the parameters at the minimum\n",
    "    iter: number of iterations taken to reach the minimum\n",
    "    grad: gradient vector at the minimum\n",
    "    '''\n",
    "    # transform numpy array inputs to torch\n",
    "    theta = torch.tensor(theta, dtype=torch.double)\n",
    "    # compute objective and derivatives at initial theta \n",
    "    f_init = f(theta)\n",
    "    j_init = torch.autograd.functional.jacobian(f, theta)\n",
    "    h_init = torch.autograd.functional.hessian(f, theta)\n",
    "    # raise an error if objective or derivatives are infinite\n",
    "    if torch.isfinite(f_init) == False:\n",
    "        raise ValueError('objective is not finite')\n",
    "    if torch.all(torch.isfinite(j_init) == False)|torch.all(torch.isfinite(h_init) == False):\n",
    "        raise ValueError('derivatives are not finite')\n",
    "    # record values of objective function, theta and iterations number \n",
    "    all_f_i = [f(theta)]\n",
    "    theta_i = theta\n",
    "    iter_i = 0\n",
    "    \n",
    "    # begin minimization iteration (outer iteration)\n",
    "    for i in range(maxit):\n",
    "        # compute Jacobian and Hessian values\n",
    "        j_i = torch.autograd.functional.jacobian(f, theta_i)  \n",
    "        h_i = torch.autograd.functional.hessian(f, theta_i)\n",
    "        # define function to check if Hessian is positive definite with Cholesky\n",
    "        def pd(h):\n",
    "            try:\n",
    "                torch.linalg.cholesky(h)\n",
    "                return True\n",
    "            except torch.linalg.LinAlgError as e:\n",
    "                return False  \n",
    "        # test if Hessian is positive definite and make perturbed adjustments\n",
    "        l = 1\n",
    "        while pd(h_i) == False:\n",
    "            # compute epsilon\n",
    "            eps = 1e-8 * torch.max(torch.abs(h_i)) * l\n",
    "            # add epsilon multiply the identity matrix\n",
    "            h_i += eps * torch.eye(h_i.shape[0])\n",
    "            # keep multiplying epsilon by 10\n",
    "            l = l * 10\n",
    "        # compute Newton step with Jacobian and Hessian values \n",
    "        step = - np.linalg.solve(h_i, j_i)\n",
    "    \n",
    "        # begin step halving iteration (inner iteration)\n",
    "        for j in range(max_half):\n",
    "            # upgrade theta and objective function values\n",
    "            new_theta_i = theta_i + step\n",
    "            new_f_i = f(new_theta_i)\n",
    "            # check if Newton step reduces the objective function value\n",
    "            if (new_f_i <= all_f_i[-1]):\n",
    "                break\n",
    "            # halve the Newton step    \n",
    "            step /= 2\n",
    "        # raise an error if Newton step fails to reduce the objective function value\n",
    "        if (new_f_i > all_f_i[-1]):\n",
    "            raise RuntimeError('fail to reduce the objective with maximum step halving number')\n",
    "        \n",
    "        # upgrade values of parameters, objective function, iterations number and Jacobian\n",
    "        theta_i, f_i = new_theta_i, new_f_i\n",
    "        all_f_i.append(f_i)\n",
    "        iter_i = iter_i + 1   \n",
    "        j_i = torch.autograd.functional.jacobian(f, theta_i) \n",
    "        # check condition for convergence\n",
    "        if (max(abs(j_i))<(abs(f_i)+fscale)*tol):\n",
    "            break\n",
    "    # raise an error if failing to converge\n",
    "    if (max(abs(j_i))>=(abs(f_i)+fscale)*tol):\n",
    "        raise RuntimeError('fail to converge with maximum iterations')\n",
    "    # raise an error if Hessian is not positive definite at convergence\n",
    "    if (pd(h_i) == False) and (max(abs(j_i))<(abs(f_i)+fscale)*tol):\n",
    "        raise ValueError('Hessian is not positive definite at convergence')\n",
    "    # create a dictionary containing all outputs  \n",
    "    dic = {'f':f_i, 'theta':theta_i, 'iter':iter_i, 'grad':j_i}\n",
    "    \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Minimization examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For three minimization problems: Quadratic function, Rosenbrock's function and Poisson regression likelihood, I first implemented the function, and then utilized my `newton()` function to find the minima using at least 3 different theta starting values. In task 1, I already ensured that all the functions will take PyTorch `Tensors` as inputs and return a `Tensor`, so I do not need to set data types as `Tensor` again in the following minimization process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Quadratic function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Objective function implementation\n",
    "## implement Quadratic function\n",
    "def qua(x):\n",
    "    return x[0]**2 - 2*x[0] + 2*x[1]**2 + x[1] + 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f': tensor(1.8750, dtype=torch.float64),\n",
       " 'theta': tensor([ 1.0000, -0.2500], dtype=torch.float64),\n",
       " 'iter': 1,\n",
       " 'grad': tensor([0., 0.], dtype=torch.float64)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## theta 1 minimization\n",
    "newton([1,2], qua, tol=1e-8, fscale=1.0, maxit=100, max_half=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f': tensor(1.8750, dtype=torch.float64),\n",
       " 'theta': tensor([ 1.0000, -0.2500], dtype=torch.float64),\n",
       " 'iter': 1,\n",
       " 'grad': tensor([0., 0.], dtype=torch.float64)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## theta 2 minimization\n",
    "newton([5,7], qua, tol=1e-8, fscale=1.0, maxit=100, max_half=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f': tensor(1.8750, dtype=torch.float64),\n",
       " 'theta': tensor([ 1.0000, -0.2500], dtype=torch.float64),\n",
       " 'iter': 1,\n",
       " 'grad': tensor([0., 0.], dtype=torch.float64)}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## theta 3 minimization\n",
    "newton([20,30], qua, tol=1e-8, fscale=1.0, maxit=100, max_half=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Rosenbrock's function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Objective function implementation\n",
    "## implement Rosenbrock's function\n",
    "def rosen(x):\n",
    "    return 10*(x[1] - x[0]**2)**2 + (1 - x[0])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f': tensor(1.1703e-23, dtype=torch.float64),\n",
       " 'theta': tensor([1.0000, 1.0000], dtype=torch.float64),\n",
       " 'iter': 7,\n",
       " 'grad': tensor([ 1.2867e-11, -3.0465e-12], dtype=torch.float64)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## theta 1 minimization\n",
    "newton([1,2], rosen, tol=1e-8, fscale=1.0, maxit=100, max_half=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f': tensor(0., dtype=torch.float64),\n",
       " 'theta': tensor([1., 1.], dtype=torch.float64),\n",
       " 'iter': 17,\n",
       " 'grad': tensor([0., 0.], dtype=torch.float64)}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## theta 2 minimization\n",
    "newton([5,7], rosen, tol=1e-8, fscale=1.0, maxit=100, max_half=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f': tensor(9.0855e-25, dtype=torch.float64),\n",
       " 'theta': tensor([1.0000, 1.0000], dtype=torch.float64),\n",
       " 'iter': 37,\n",
       " 'grad': tensor([ 1.1436e-11, -5.2491e-12], dtype=torch.float64)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## theta 3 minimization\n",
    "newton([20,30], rosen, tol=1e-8, fscale=1.0, maxit=100, max_half=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Poisson regression likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## record original data \n",
    "x = [\n",
    "   0.11, -0.06, -0.96, -0.48, -0.59, -0.42, -0.15,  1.14, 0.94, \n",
    "  -0.86, -0.08,  1.00, -2.01,  2.17, -0.20,  0.82, -0.13, 0.26, \n",
    "   0.22,  1.05\n",
    "]\n",
    "\n",
    "y = [4, 2, 4, 1, 1, 3, 4, 5, 7, 3, 5, 7, 0, 4, 2, 7, 3, 3, 2, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Objective function implementation\n",
    "## implement log-likelihood function for Poisson regression \n",
    "def poisson(b):\n",
    "    log_l = b[0] + b[1]*torch.tensor(x,dtype=torch.double)\n",
    "    l = torch.exp(log_l)\n",
    "    s = torch.sum(torch.mul(torch.tensor(y,dtype=torch.double),log_l)-l-torch.log(torch.tensor([math.factorial(m) for m in y])))\n",
    "    return -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f': tensor(37.8802, dtype=torch.float64),\n",
       " 'theta': tensor([1.2089, 0.4279], dtype=torch.float64),\n",
       " 'iter': 6,\n",
       " 'grad': tensor([4.8569e-06, 6.8655e-06], dtype=torch.float64)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## theta 1 minimization\n",
    "newton([1,2], poisson, tol=1e-6, fscale=30.0, maxit=100, max_half=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f': tensor(37.8802, dtype=torch.float64),\n",
       " 'theta': tensor([1.2089, 0.4279], dtype=torch.float64),\n",
       " 'iter': 21,\n",
       " 'grad': tensor([2.6478e-07, 3.7425e-07], dtype=torch.float64)}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## theta 2 minimization\n",
    "newton([5,7], poisson, tol=1e-6, fscale=30.0, maxit=100, max_half=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f': tensor(37.8802, dtype=torch.float64),\n",
       " 'theta': tensor([1.2089, 0.4279], dtype=torch.float64),\n",
       " 'iter': 86,\n",
       " 'grad': tensor([7.5907e-08, 1.0729e-07], dtype=torch.float64)}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## theta 3 minimization\n",
    "newton([20,30], poisson, tol=1e-6, fscale=30.0, maxit=100, max_half=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
